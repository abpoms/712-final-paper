% --------------- 12 POINT FONT -------------------------------
\documentclass[tog]{acmsiggraph}

% --------------- 10 POINT FONT FOR CAPTIONS ------------------

\usepackage{listings}
\usepackage[dvipsnames]{xcolor}
%%% Make the ``BibTeX'' word pretty...

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

%%% Used by the ``review'' variation; the online ID will be printed on 
%%% every page of the content.

\TOGonlineid{45678}

%%% Used by the ``preprint'' variation.

\TOGvolume{0}
\TOGnumber{0}

% --------------- MATH PACKAGES -------------------------------
\usepackage{amsmath,amsthm,amssymb}

\title{Processing collections of images and videos on big-data frameworks}
\author{Alex Poms, Ravi Teja Mullapudi, \\Suhail Rehman}
\pdfauthor{Alex Poms, Ravi Teja Mullapudi, \\Suhail Rehman}

\begin{document}


\maketitle

\begin{abstract}
\end{abstract}

% --------------- CONTENT -------------------------------------
\section{Introduction}
Images and video already dominate internet traffic and occupy a significant
amount of datacenter storage. The Cisco Visual Networking Index predicts that
IP video traffic will consume over 80\% of total internet bandwidth in 2019. In
2009, Facebook reported 1.5PB of image storage with 25TB added weekly and
550,000 images served per second at peak. This visual data is mostly just
served directly to a human user for viewing. However, recent advances in
computer vision, such as feasible image/object understanding and reconstruction
of 3D structure from image collections, motivate large-scale analysis of images
and videos for deriving insights and information that is not accessible from
other sources. For example, a relief organization could automatically
reconstruct the damage done by a natural disaster by having vehicles with
mounted cameras observe the affected area or a state could perform automatic
demographics estimation by analyzing the public Facebook profile photos of
their residents.

Currently, visual computing applications are few and far between due to the
inherent complexities of working with large visual datasets. The most apparent
of these difficulties is that the massive size of the datasets vastly exceed
the storage and computational capacity of a single machine. Thus any
application must be distributed and parallelized across a large cluster,
managing data transfer and communication between nodes. Unlike traditional
analytics workloads, visual computing applications also require highly tuned
and efficient image processing kernels that take advantage of the heterogeneous
hardware increasingly seen in cloud infrastructure, such as throughput-oriented
accelerators including GPUs and even FPGAs. Having a predefined set of
optimized implementations is not enough because image processing pipelines
require global code restructuring that crosses function boundaries; each
application may require custom image processing code that is composed with
other functionality. Lastly, visual computing applications exhibit tight
coupling between traditional analytics operations, such as those of Spark
\cite{zaharia2012resilient} and MapReduce \cite{dean2008mapreduce}, and
non-streaming operations, like distributed optimization or graph processing.

It is unclear if existing systems can simultaneously satisfy these three
requirements. Systems optimized for analytics operations, like Spark, tend to
sacrifice explicit access to their data collections that prohibit efficient
integration of non-streaming operations. Analytics systems also lack the
ability to specialize user routines to heterogeneous hardware resources. On the
other hand, systems that perform non-streaming operations, like the Parameter
Server \cite{li2014scaling}, generally lack the flexibility in their
programming model to allow direct incorporation of streaming operations like
map, filter, or reduce.

\section{Background}

\subsection{Spark}

\subsection{Legion}

\subsection{SciDB}


\section{Workloads}
Visual data applications contain a diverse set of operations, but a few of them
stand out as particularly important and distinctly core components of the
analysis algorithms they are a part of. In particular, two classes
of operations that seem to span a significant amount of the space of important
operations in visual data applications are: very compute-intensive kernels
applied individually across an entire dataset, and aggregate,
high-communication operations, like computing nearest neighbors or
clustering. For a distributed computing framework to serve as a platform for
creating performant applications that process large collections of images and
videos, it must at least support very efficient and scalable implementations of
the following operations:

\subsection{High-performance map kernel}

\subsection{Clustering}

%\subsection{Filter clustering}

\section{Distributed collections on Legion}

\subsection{}

\subsection{Task scheduling}

\subsection{KMeans Implementation}

\section{Evaluation}

\subsection{Spark vs Legion}

\subsubsection{Partition startup time}

\subsection{SciDB vs MPI}

\section{Discussion}

\subsection{Native code on Spark}

\subsection{Improving distributed collection abstractions}

\subsubsection{Batch size}

\subsubsection{Iterators}

\subsection{Improving SciDB}

\section{Conclusions}

\bibliographystyle{acmsiggraph}
\nocite{*}
\bibliography{final}

\end{document}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
